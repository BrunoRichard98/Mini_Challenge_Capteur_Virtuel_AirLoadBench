{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travail developpe par le binome :\n",
    "**Bruno OLIVEIRA, Samuel GHEZI**\n",
    "Equipe 07\n",
    "\n",
    "Sur orientation de le Professeur **Martin GHIENNE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "L’exploitation d’un avion s’accompagne de conditions de vol très variables et difficiles à prédire, ce qui rend complexe l’estimation précise des chargements structuraux rencontrés en situation réelle. Bien que des informations telles que les déformations et les contraintes soient essentielles pour optimiser la maintenance et améliorer les modèles de dimensionnement, ces grandeurs ne sont généralement pas mesurées directement sur les aéronefs commerciaux. L’installation de capteurs dédiés entraînerait en effet une augmentation significative des coûts, de la masse, de la complexité d’intégration et des exigences de certification.\n",
    "\n",
    "Le Mini-Challenge propose ainsi de développer un capteur virtuel basé sur des méthodes d’apprentissage automatique, capable d’estimer l’état de contrainte structurelle en différents points de l’avion à partir des seuls paramètres déjà enregistrés par l’instrumentation de bord. L’objectif est de prédire des grandeurs non mesurées physiquement, mais inférées à partir de variables de vol telles que l’attitude, les vitesses, les accélérations, les ordres de commande et les conditions de vent.\n",
    "\n",
    "Pour cela, un ensemble de données réelles provenant de 44 vols d’essai est mis à disposition. Ce jeu de données comprend :\n",
    "\n",
    "39 paramètres issus de l’instrumentation de bord, représentant l’état de vol, les efforts aérodynamiques et les actions de contrôle ;\n",
    "\n",
    "15 jauges d’extensométrie (en micro-déformations, με) positionnées en différents points structuraux de l’appareil, permettant de mesurer directement les contraintes locales.\n",
    "\n",
    "En résumé, ce projet vise à démontrer la capacité d’un modèle d’apprentissage supervisé à reproduire les contraintes structurelles réelles à partir de données opérationnelles courantes, ouvrant la voie à des stratégies de maintenance plus prédictives, moins coûteuses et mieux informées, dans la continuité des travaux précédents sur les capteurs virtuels pour le suivi de santé structurale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 études des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Lybraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Management des données** - Acquisition des données -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the .csv file with all the Tols, and creating the standard dataframe with all Tols concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframes data[Tol_1], ..., data[Tol_48]\n",
    "data = {}\n",
    "for file in glob.glob('./Data_AirLoadBench/TOL_*.csv'):\n",
    "    base = os.path.basename(file)\n",
    "    name = base.replace('.csv', '')\n",
    "    data[name]  = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vizualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For vizualise the correlation between the outputs with all the features we've use the notebook below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Sélection du DataFrame principal\n",
    "# # Listes des colonnes (Y = variables de vol, X = strains)\n",
    "\n",
    "# y_cols = [\n",
    "#     'Relative_Time', 'Nz', 'Nx', 'Roll_Angle', 'Pitch_Angle', 'True_AOA',\n",
    "#     'True_Sideslip', 'FPA', 'True_Heading', 'CAS', 'TAS', 'Mach', 'SAT',\n",
    "#     'Baro_Alt', 'Roll_Rate', 'Pitch_Rate', 'Heading_Rate', 'Fuel_Qty1',\n",
    "#     'Fuel_Qty2', 'L_Eng_Start', 'R_Eng_Start', 'L_Throttle_Pos',\n",
    "#     'R_Throttle_Pos', 'L_Eng_N1', 'R_Eng_N1', 'L_Eng_N2', 'R_Eng_N2',\n",
    "#     'L_Gear_Down', 'R_Gear_Down', 'N_Gear_Down', 'L_Flaperon_Pos',\n",
    "#     'R_Flaperon_Pos', 'L_LEF_Pos', 'R_LEF_Pos', 'L_Rudder_Pos',\n",
    "#     'L_Stab_Pos', 'R_Stab_Pos', 'Stick_Pitch', 'Stick_Roll', 'Pedal_Pos'\n",
    "# ]\n",
    "\n",
    "# x_cols = [\n",
    "#     'Strain1', 'Strain2', 'Strain3', 'Strain4', 'Strain5', 'Strain6',\n",
    "#     'Strain7', 'Strain8', 'Strain9', 'Strain10', 'Strain11', 'Strain12',\n",
    "#     'Strain13', 'Strain14', 'Strain15'\n",
    "# ]\n",
    "\n",
    "# #  Conversion des colonnes en valeurs numérique \n",
    "# df_numeric = df.copy()\n",
    "\n",
    "# for col in y_cols + x_cols:\n",
    "#     if col in df_numeric.columns:  # sécurité si une colonne manque\n",
    "#         df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# #  Génération des graphiques de dispersion (scatter)\n",
    "# #    Pour chaque strain, une figure contenant plusieurs sous-graphiques\n",
    "# #    Axe X = strain, Axe Y = variable de vol\n",
    "# for strain in x_cols:\n",
    "\n",
    "#     if strain not in df_numeric.columns:\n",
    "#         print(f\"La colonne {strain} n'est pas présente dans le DataFrame, ignorée.\")\n",
    "#         continue\n",
    "\n",
    "#     # grille 7x6 de sous-graphiques (42 cases, nous avons 41 variables Y)\n",
    "#     fig, axes = plt.subplots(\n",
    "#         nrows=7, \n",
    "#         ncols=6, \n",
    "#         figsize=(18, 20),\n",
    "#         sharex=False, \n",
    "#         sharey=False\n",
    "#     )\n",
    "#     axes = axes.ravel()  # transforme la grille en liste linéaire\n",
    "\n",
    "#     # Pour chaque variable Y, créer un scatter plot\n",
    "#     for i, y in enumerate(y_cols):\n",
    "\n",
    "#         if y not in df_numeric.columns:\n",
    "#             print(f\"La colonne {y} n'est pas présente dans le DataFrame, ignorée.\")\n",
    "#             continue\n",
    "\n",
    "#         ax = axes[i]\n",
    "\n",
    "#         ax.scatter(\n",
    "#             df_numeric[strain],\n",
    "#             df_numeric[y],\n",
    "#             s=5,\n",
    "#             alpha=0.5\n",
    "#         )\n",
    "#         ax.set_xlabel(strain, fontsize=8)\n",
    "#         ax.set_ylabel(y, fontsize=8)\n",
    "#         ax.tick_params(labelsize=8)\n",
    "\n",
    "#     # Suppression des sous-graphiques inutilisés\n",
    "#     for j in range(len(y_cols), len(axes)):\n",
    "#         fig.delaxes(axes[j])\n",
    "\n",
    "#     # Titre global de la figure\n",
    "#     fig.suptitle(\n",
    "#         f'Relation entre {strain} et les variables de vol (Données : TOL_1)',\n",
    "#         fontsize=16\n",
    "#     )\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #  Heatmap des corrélations (optionnel mais très utile)\n",
    "# #  Visualisation de la corrélation entre toutes les Y et toutes les Strains\n",
    "\n",
    "# cols_present = [c for c in (y_cols + x_cols) if c in df_numeric.columns]\n",
    "\n",
    "# # matrice de corrélation\n",
    "# corr = df_numeric[cols_present].corr()\n",
    "\n",
    "# # sous-matrice : lignes = Y, colonnes = Strains\n",
    "# corr_sub = corr.loc[\n",
    "#     [c for c in y_cols if c in corr.index],\n",
    "#     [c for c in x_cols if c in corr.columns]\n",
    "# ]\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(\n",
    "#     corr_sub,\n",
    "#     annot=True,\n",
    "#     fmt=\".2f\",\n",
    "#     cmap='coolwarm',\n",
    "#     center=0,\n",
    "#     annot_kws={\"size\": 8} \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 16):\n",
    "#     filename = f\"{base_path}scatter_Strain{i}.png\"\n",
    "    \n",
    "#     img = mpimg.imread(filename)\n",
    "#     plt.figure(figsize=(20, 12))\n",
    "#     plt.imshow(img)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(f\"scatter_Strain{i}.png\", fontsize=20)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data frame creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    df_temp = df.copy()\n",
    "    df_temp[\"Tol_ID\"] = name\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# junta tudo em um dataframe só\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='Relative_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame Filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Frame Filtered** : We created another data frame with filter strategies with the goal to analyse the results and compare, we let it here to show our intention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_cols = [f\"Strain{i}\" for i in range(1, 16)]\n",
    "# feature_cols = [c for c in df.columns if c not in target_cols + [\"Tol_ID\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the representation of the aircraft’s total fuel load, the variables Fuel_Qty1 and Fuel_Qty2 were merged into a single feature, Fuel, defined as the sum of both tank quantities. This transformation removes redundancy in the dataset and preserves the information that is most relevant for modeling structural loads, as the total fuel mass has a greater impact on aircraft dynamics than the individual tank values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Fuel'] = df['Fuel_Qty1'] + df['Fuel_Qty2']\n",
    "# df = df.drop(columns = ['Fuel_Qty1','Fuel_Qty2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the overall intensity of the aircraft’s longitudinal and vertical load factors, the variables Nx and Nz were merged into a single feature representing their combined magnitude. This derived variable provides a more compact and physically meaningful measure of the total load acting on the structure, helping reduce redundancy while retaining the information most relevant to predicting strain and structural behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['N_Load_Magnitude'] = np.sqrt(df['Nz']**2 + df['Nx']**2)\n",
    "# df = df.drop(columns = ['Nz','Nx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables Stick_Pitch and Stick_Roll represent the pilot’s longitudinal and lateral control inputs, respectively. While each axis contributes independently to specific aircraft motions, the overall control effort applied by the pilot is best captured by their combined magnitude. By merging these two inputs into a single variable, Stick_Intensity, we obtain a more compact and physically meaningful representation of the pilot’s control activity.\n",
    "\n",
    "This new feature reflects the total magnitude of the stick deflection, regardless of direction, which is closely related to the aircraft’s dynamic response and the resulting structural loads. Combining these inputs helps reduce redundancy, improves model interpretability, and emphasizes the intensity of the pilot’s command rather than treating pitch and roll inputs independently, which may be less relevant for strain prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Stick_Magnitude'] = np.sqrt(df['Stick_Pitch']**2 + df['Stick_Pitch']**2)\n",
    "# df = df.drop(columns = ['Stick_Pitch', 'Stick_Pitch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of strain prediction during landing, True Airspeed (TAS) provides the most physically meaningful representation of the aircraft’s aerodynamic state. TAS reflects the actual velocity of the aircraft relative to the surrounding air mass, which directly influences aerodynamic loads, structural response, and the resulting strain measurements.\n",
    "\n",
    "Calibrated Airspeed (CAS) is primarily useful for instrumentation and low-speed performance considerations, but it does not add meaningful information once TAS is available. Similarly, Mach number represents the ratio of airspeed to the speed of sound, making it more relevant at high-speed or high-altitude flight regimes. During landing, Mach variations are small and largely determined by temperature an effect already captured through other variables such as SAT.\n",
    "\n",
    "Therefore, retaining TAS while removing CAS and Mach reduces redundancy and preserves the variable that best reflects the aerodynamic loads relevant to structural strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns = ['CAS','Mach'])\n",
    "# target_cols = [f\"Strain{i}\" for i in range(1, 16)]\n",
    "# feature_cols = [c for c in df.columns if c not in target_cols + [\"Tol_ID\"]]\n",
    "\n",
    "# df_clean = df.dropna(subset=feature_cols + target_cols)\n",
    "\n",
    "# X = df_clean[feature_cols]   # Inputs\n",
    "# y = df_clean[target_cols]    # Outputs (strains)df_clean = df.dropna(subset=feature_cols + target_cols)\n",
    "\n",
    "# X = df_clean[feature_cols]   # Inputs\n",
    "# y = df_clean[target_cols]    # Outputs (strains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step combines all input features and target variables into a single dataset and computes the correlation matrix for every variable. From this matrix, we extract only the correlations between the selected features and the target variables. The resulting table allows us to clearly identify how strongly each feature is linearly related to each output. This is an essential step in the preprocessing stage, as it helps evaluate the relevance of each input variable and supports decisions about removing features with weak or insignificant correlation to the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_full = pd.concat([X, y], axis=1).corr()\n",
    "# corr_feat_target = corr_full.loc[feature_cols, target_cols]\n",
    "# corr_feat_target\n",
    "\n",
    "# threshold = 0.2\n",
    "\n",
    "# selected_features = corr_abs[corr_abs.max(axis=1) >= threshold].index.tolist()\n",
    "# print(\"Features selected for threshold:\")\n",
    "# print(selected_features)\n",
    "\n",
    "# removed_features = list(set(feature_cols) - set(selected_features))\n",
    "# print(\"Features removed for low correlation:\")\n",
    "# print(removed_features)\n",
    "\n",
    "# SORTIE DU NOTEBOOK : \n",
    "# Features selected for threshold:\n",
    "# ['Pitch_Angle', 'True_AOA', 'TAS', 'SAT', 'Baro_Alt', 'Pitch_Rate', 'L_Throttle_Pos', 'R_Throttle_Pos', 'L_Eng_N1', 'R_Eng_N1', 'L_Eng_N2', 'R_Eng_N2', 'L_Gear_Down', 'R_Gear_Down', 'N_Gear_Down', 'L_Flaperon_Pos', 'R_Flaperon_Pos', 'L_LEF_Pos', 'R_LEF_Pos', 'L_Stab_Pos', 'R_Stab_Pos', 'Stick_Roll', 'Fuel', 'N_Load_Magnitude', 'Stick_Magnitude']\n",
    "# Features removed for low correlation:\n",
    "# ['L_Eng_Start', 'Roll_Rate', 'R_Eng_Start', 'L_Rudder_Pos', 'True_Sideslip', 'Pedal_Pos', 'Heading_Rate', 'Roll_Angle', 'FPA', 'True_Heading']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Justification for Removing Low-Correlation Features\n",
    "\n",
    "The variables excluded from the dataset showed correlation values below the 0.20 threshold and have limited physical relevance to structural strain during landing. Several of them describe yaw-related phenomena such as *True_Sideslip*, *L_Rudder_Pos*, *Pedal_Pos*, *Heading_Rate*, and *Roll_Rate*—which exert minimal influence on vertical and longitudinal structural loads, the primary contributors to strain in the aircraft’s wings, fuselage, and landing gear attachments. During the landing phase, yaw motion is typically small and mainly used for runway alignment, resulting in negligible structural deformation.\n",
    "\n",
    "Other removed variables, including *L_Eng_Start* and *R_Eng_Start*, represent engine state rather than forces or dynamic behavior and therefore provide no meaningful information for predicting strain. Additionally, *Roll_Angle* and *FPA* were excluded due to their weak direct contribution to strain and redundancy with stronger predictors such as pitch angle and angle of attack.\n",
    "\n",
    "By eliminating these low-impact and redundant features, the model retains only the variables that exhibit a strong physical and statistical relationship with structural loads, improving model clarity, training efficiency, and predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars_to_remove = [\n",
    "#     'True_Sideslip',\n",
    "#     'L_Eng_Start',\n",
    "#     'R_Eng_Start',\n",
    "#     'Roll_Angle',\n",
    "#     'L_Rudder_Pos',\n",
    "#     'Pedal_Pos',\n",
    "#     'Heading_Rate',\n",
    "#     'Roll_Rate',\n",
    "#     'True_Heading',\n",
    "#     'FPA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns = vars_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols_filtered = [c for c in df.columns if c not in target_cols + [\"Tol_ID\"]]\n",
    "\n",
    "# print(\"Features:\", feature_cols_filtered)\n",
    "# print(\"Targets :\", target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[feature_cols_filtered].values      # inputs\n",
    "# y = df[target_cols].values                # outputs (strains)\n",
    "# Tol_ids = df_clean[\"Tol_ID\"].values  # Relationed to wich TOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all these steps, we will have train_test_split, we are going to show it better in the following notebooks, but this strategy it was not the best choice. we've trained with MLP model with 30 epochs and the results with the same model and same epochs number with all the variables it was better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing with the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets = deformations (outputs of the model)\n",
    "target_cols = [f\"Strain{i}\" for i in range(1, 16)]\n",
    "\n",
    "# Features = strain columns\n",
    "feature_cols = [c for c in df.columns if c not in target_cols + [\"Tol_ID\"]]\n",
    "\n",
    "print(\"Features:\", feature_cols)\n",
    "print(\"Targets :\", target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NnN numbers lignes\n",
    "df_clean = df.dropna(subset=feature_cols + target_cols)\n",
    "\n",
    "X = df_clean[feature_cols].values      # inputs\n",
    "y = df_clean[target_cols].values      # putputs (strains)\n",
    "Tol_ids = df_clean[\"Tol_ID\"].values  # TOL related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset et dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_temp, y_train, y_temp, ids_train, ids_temp = train_test_split(\n",
    "    X, y, Tol_ids,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=Tol_ids\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test, ids_val, ids_test = train_test_split(\n",
    "    X_temp, y_temp, ids_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=ids_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled   = scaler_X.transform(X_val)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled   = scaler_y.transform(y_val)\n",
    "y_test_scaled  = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrainDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = StrainDataset(X_train_scaled, y_train_scaled)\n",
    "val_ds   = StrainDataset(X_val_scaled,   y_val_scaled)\n",
    "test_ds  = StrainDataset(X_test_scaled,  y_test_scaled)\n",
    "\n",
    "train_loader_mlp = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader_mlp   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
    "test_loader_mlp  = DataLoader(test_ds,  batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre fonction `StrainDataset_multi` similaire est créée pour le multi modèle. Voulant lier nos variables au temps, on souhaite maintenant avoir un dataloader qui délivre des séquences de plusieurs pas de temps. Sequence len sera initialisé à `seuquence_len1 = 20` pour un premier essai. Sachant que nos pas de temps sont de 0.032 secondes, il serait judicieux de prendre un sequence len plus important : `sequence_len2 = 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StrainDataset_multi(Dataset):\n",
    "#     def __init__(self, X, y, seq_len):\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "#         self.seq_len = seq_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X) - self.seq_len + 1\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x_seq = self.X[idx : idx + self.seq_len]\n",
    "#         y_seq = self.y[idx : idx + self.seq_len]\n",
    "#         return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "# sequence_len1 = 20\n",
    "# sequence_len2 = 100\n",
    "\n",
    "# #Multi_model\n",
    "\n",
    "# train_ds = StrainDataset_multi(X_train_scaled, y_train_scaled, sequence_len1)\n",
    "# val_ds   = StrainDataset_multi(X_val_scaled,   y_val_scaled, sequence_len1)\n",
    "# test_ds  = StrainDataset_multi(X_test_scaled,  y_test_scaled, sequence_len1)\n",
    "\n",
    "# train_loader_multimodel = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "# val_loader_multimodel   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
    "# test_loader_multimodel  = DataLoader(test_ds,  batch_size=256, shuffle=False)\n",
    "\n",
    "# #Multi_model_opti\n",
    "\n",
    "# train_ds = StrainDataset_multi(X_train_scaled, y_train_scaled, sequence_len2)\n",
    "# val_ds   = StrainDataset_multi(X_val_scaled,   y_val_scaled, sequence_len2)\n",
    "# test_ds  = StrainDataset_multi(X_test_scaled,  y_test_scaled, sequence_len2)\n",
    "\n",
    "# train_loader_multimodel_opti = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "# val_loader_multimodel_opti   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
    "# test_loader_multimodel_opti  = DataLoader(test_ds,  batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[-1]   # 39 features\n",
    "n_outputs  = y_train_scaled.shape[-1]   # 15 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs architectures seront proposées au cours de notre recherche afin de déterminer laquelle répond au mieux à nos attentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MLP Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commencerons par un réseau de neuronnes classique MLP. Nous choisissons tout d'abord une vision \"statique\" de notre modélisation de capteur, sans prendre en compte certaine notion de temps que pourrait avoir certaines valeurs physiques de nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StrainMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Multi-modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre problème est dynamique car différentes variables (dans les données) dépendent du temps. Comme les variables sont nombreuses il serait aussi intéressant de réduire nos nombres de variables afin de les caractériser au mieux. Ainsi, la solution choisit serait un `Multi-modèle` :\n",
    "- En premier lieu, nous allons réduire nos variables en un espace latent z avec un `Encodeur` qui caractérise au mieux \"spatialement\" notre problème.\n",
    "- Nous souhaitons aussi avoir des \"fenêtres temporelles d'informations\" afin de relier certaine grandeur physiques au temps. Ainsi, la seconde \"couche\" de notre modèle seront des couches `LSTM` qui relie nos informations au temps, et qui rend un tenseur avec les derniers état caché de chaque cellule.\n",
    "- Finalement, nos informations passe par des dernières couches d'un `MLP` linéaire standard qui permettent la prédiction des contraintes appliquée à notre avion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Encoder layers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):#réduction de nos variable en un espace latent z qui caractérise au mieux nos données.\n",
    "#     def __init__(self, input_size, z_dim):\n",
    "#         super().__init__() #on appelle la classe parent Module\n",
    "#         self.hidden1 = nn.Linear(input_size, 64) #chaque couche cachée comporte un nombre pour l'instant instancié au \"hasard\"\n",
    "#         kaiming_uniform_(self.hidden1.weight, nonlinearity='relu') #on utilise la distribution uniforme de kaiming pour initialiser nos poids\n",
    "#         self.hidden2 = nn.Linear(64, 32)\n",
    "#         kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "#         self.hidden3 = nn.Linear(32, z_dim)#le tenseur de sortie est l'espace latent, il décrit nos variables avec le minimum de caractéristique que l'on lui a inculqué.\n",
    "#         kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "#         self.relu = nn.ReLU() #la fonction reLU a le gradiant le plus stable, elle semble donc être un bon choix pour notre projet.\n",
    "#     def forward (self, x): #passage du tenseur dans les différentes couches de notre Encoder\n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hidden2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hidden3(x)\n",
    "#         # x = self.relu(x)\n",
    "#         return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LSTM layers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Lstm(nn.Module):\n",
    "#     def __init__(self, input_size: int, hidden_size: int, nb_layers: int, dropout: float = 0.1):\n",
    "#         \"\"\"\n",
    "#             on initialise notre système avec un dropout faible, qui permet de désactiver certaine cellules avec un taux d'activation à 10%\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.input_size = input_size # récupère la taille des vecteurs des valeurs d'entrées\n",
    "#         self.hidden_size = hidden_size  # récupère le nombre de cellules par couche de lstm\n",
    "#         self.nb_layers = nb_layers  # récupère le nombre de couche dans le LSTM\n",
    "#         # Défini les couches LSTM\n",
    "#         self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=nb_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # on fait passer l'entrée par les différente cellules du lstm\n",
    "#         output, (h_n, c_n) = self.lstm(x)\n",
    "#         return output #on récupère tout les états cachés de du passage de la séquence de variable et pas seulement le vecteur hn.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MLP layers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP_v2(nn.Module):\n",
    "#     def __init__(self, input_size, n_contraintes):\n",
    "#         super().__init__()# ici on rajoute un mlp qui récupère les états caché en entrée et qui prédit les 15 contraintes.\n",
    "#         self.hidden1 = nn.Linear(input_size, 256)#on instancie nos valeurs encore une fois sans exactement savoir pour l'instant quel sont les hyperparamètres les plus concluants\n",
    "#         kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "#         self.hidden2 = nn.Linear(256, 128)\n",
    "#         kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "#         self.hidden3 = nn.Linear(128, n_contraintes)\n",
    "#         kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "#         self.relu = nn.ReLU()\n",
    "#     def forward(self, x):\n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hidden2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hidden3(x) # biensur la sortie de la dernière couche cachée de perceptrons de passera pas par une fonction d'activation ReLU, ne voulant pas supprimier des valeurs négatives.\n",
    "#         return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Multi modele***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mercosur(nn.Module):\n",
    "#     def __init__(self, input_size, z_dim, hidden_size, nb_layers, n_contraintes):\n",
    "#         super().__init__() # on instancie chaque partie du multi modèle\n",
    "#         self.encoder_layers = Encoder(input_size, z_dim)\n",
    "#         self.lstm_layers = Lstm(z_dim, hidden_size, nb_layers)\n",
    "#         # self.mlp_layers = MLP(hidden_size, n_contraintes).\n",
    "#         self.mlp_layers = MLP_v2(hidden_size, n_contraintes)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch, seq_len, n_features)\n",
    "#         z_seq = self.encoder_layers(x)      # (batch, seq_len, z_dim)\n",
    "#         h_seq = self.lstm_layers(z_seq)     # (batch, seq_len, hidden_size)\n",
    "#         y_pred = self.mlp_layers(h_seq)     # (batch, seq_len, n_contraintes)\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Multi-modèle optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "à l'inverse du premier modèle instancié au hasard, on propose ici un mlti modèle \"variable\"(avec des valeurs par défaut si non précisées). on pourra part la suite choisir des intervalles de recherches pour nos hyper paramètre avec un optimiseur d'hyperparamètre. Celui choisit pour l'étude sera `Optuna`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Encoder***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder_opti(nn.Module):#réduction de nos variable en un espace latent z qui caractérise au mieux nos données.\n",
    "#     def __init__(self, input_size, nb_percp=64, nb_layers=3, z_dim = 16):\n",
    "#         super().__init__() #on appelle la classe parent Module\n",
    "#         layers = []\n",
    "#         in_dim = input_size\n",
    "#         for _ in range(nb_layers): #comme le nombre de couches est inconnu, il est donc nécessaire d'utiliser une boucle for pour créer notre classe\n",
    "#             linear = nn.Linear(in_dim, nb_percp)\n",
    "#             kaiming_uniform_(linear.weight, nonlinearity='relu')\n",
    "#             layers.append(linear)#on ajoute la couche linéaire à une liste qui correspond à notre réseau caché - la dernière couche\n",
    "#             layers.append(nn.ReLU()) #sans oublier les fonction d'activation\n",
    "#             in_dim = nb_percp # après être passé une première fois dans la boucle, nos couches auront le même nombres de neuronnes\n",
    "#         output = nn.Linear(in_dim, z_dim) \n",
    "#         kaiming_uniform_(output.weight, nonlinearity='relu')\n",
    "#         layers.append(output) #on ajoute la dernière couche de notre encoder\n",
    "#         self.net = nn.Sequential(*layers) #on appelle le pointeur de la liste pour récupérer les différentes couches et créer le \"net\" total.\n",
    "#          #la fonction reLU a le gradiant le plus stable, elle semble donc être un bon choix pour notre projet.\n",
    "#     def forward (self, x): #passage du tenseur dans les différentes couches de notre Encoder\n",
    "#         return self.net(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LSTM Layers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Lstm_opti(nn.Module):\n",
    "#     def __init__(self, input_size: int, hidden_size: int, nb_layers: int, dropout: float = 0.1):\n",
    "#         \"\"\"\n",
    "#             on initialise notre système avec un dropout faible\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.input_size = input_size # récupère la taille des vecteurs des valeurs d'entrées\n",
    "#         self.hidden_size = hidden_size  # récupère le nombre de cellules par couche de lstm\n",
    "#         self.nb_layers = nb_layers  # récupère le nombre de couche dans le LSTM\n",
    "#         # Défini les couches LSTM\n",
    "#         self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=nb_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # on fait passer l'entrée par les différente cellules du lstm\n",
    "#         output, (h_n, c_n) = self.lstm(x)\n",
    "#         return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MLP layers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP_v2_opti(nn.Module):\n",
    "#     def __init__(self, input_size, nb_percp = 128, nb_layers = 3, n_contraintes = 15):\n",
    "#         super().__init__()\n",
    "#         layers = []\n",
    "#         in_dim = input_size\n",
    "#         for _ in range(nb_layers):#on applique la même réflexion que l'encoder au mlp qui donne les différentes contraintes en sortie.\n",
    "#             linear = nn.Linear(in_dim, nb_percp)\n",
    "#             kaiming_uniform_(linear.weight, nonlinearity='relu')\n",
    "#             layers.append(linear)\n",
    "#             layers.append(nn.ReLU())\n",
    "#             in_dim = nb_percp\n",
    "#         output = nn.Linear(in_dim, n_contraintes)\n",
    "#         kaiming_uniform_(output.weight, nonlinearity='linear')\n",
    "#         layers.append(output)\n",
    "#         self.net = nn.Sequential(*layers)\n",
    "#          #la fonction reLU a le gradiant le plus stable, elle semble donc être un bon choix pour notre projet.\n",
    "#     def forward (self, x): #passage du tenseur dans les différentes couches de notre Encoder.\n",
    "#         return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Multi_model optimized***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mercosur_opti(nn.Module):\n",
    "#     def __init__(self, input_size, nb_percp_enc, nb_layers_enc, z_dim, hidden_size_lstm, nb_layers_lstm, nb_percp_mlp, nb_layers_mlp, n_contraintes):\n",
    "#         super().__init__()\n",
    "#         self.encoder_layers = Encoder_opti(input_size, nb_percp_enc, nb_layers_enc, z_dim)\n",
    "#         self.lstm_layers = Lstm_opti(z_dim, hidden_size_lstm, nb_layers_lstm)\n",
    "#         # self.mlp_layers = MLP(hidden_size, n_contraintes).\n",
    "#         self.mlp_layers = MLP_v2_opti(hidden_size_lstm, nb_percp_mlp, nb_layers_mlp, n_contraintes)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch, seq_len, n_features)\n",
    "#         z_seq = self.encoder_layers(x)      # (batch, seq_len, z_dim)\n",
    "#         h_seq = self.lstm_layers(z_seq)     # (batch, seq_len, hidden_size)\n",
    "#         y_pred = self.mlp_layers(h_seq)     # (batch, seq_len, n_contraintes)\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement et test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialisation des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Strain_mlp = StrainMLP().to(device)\n",
    "criterion_mlp = nn.MSELoss()\n",
    "optimizer_mlp = torch.optim.Adam(Strain_mlp.parameters(), lr=1e-3)\n",
    "\n",
    "#Multimodel\n",
    "# Multi_model = Mercosur(n_features, z_dim_multi, hidden_size1, nb_layers_lstm_multi, n_outputs).to(device)\n",
    "\n",
    "# criterion_Multi_model = nn.MSELoss()\n",
    "# optimizer_Multi_model = torch.optim.Adam(Multi_model.parameters(), lr=1e-3)\n",
    "\n",
    "# l'initalisation du multimodèle optimisé se font directement dans la fonction à minimiser avec la librairie optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "information pour le multi modèle :afin de bien le caractériser sans utiliser d'optimiseur, il pourrait être judicieux de connaître quelles sont les variables qui se retrouve dans plusieurs phénomène physiques des données récupérées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pour le multi modèle\n",
    "\n",
    "# hidden_size1 = 64 # le réseau traite beaucoup d'informations et ce sur une séquence de temps qui multiplie les liens entre eux. il faut donc un nombre de cellule dans le lstm équivalent\n",
    "# #le nombre d'entrée du MLP qui servira \n",
    "\n",
    "# z_dim_multi = 16 #encore une fois le choix de la taille du vecteur de l'espace latent est déterminé aléatoirement.\n",
    "# nb_layers_lstm_multi = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in tqdm(loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = train_epoch(Strain_mlp, train_loader_mlp, optimizer_mlp, criterion_mlp)\n",
    "    val_loss   = eval_epoch(Strain_mlp, val_loader_mlp, criterion_mlp)\n",
    "    print(f\"Epoch {epoch:03d} | Train loss: {train_loss:.6f} | Val loss: {val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, loader, optimizer, criterion): # on instancie ici l'entraînement\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for i, (X_batch, y_batch) in enumerate(tqdm(loader)):\n",
    "#         # \n",
    "#         if i == 0:\n",
    "#             print(\"X_batch shape:\", X_batch.shape)\n",
    "#             print(\"y_batch shape:\", y_batch.shape)\n",
    "\n",
    "#         X_batch = X_batch.to(device)\n",
    "#         y_batch = y_batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "#     return running_loss / len(loader.dataset)\n",
    "\n",
    "# def eval_epoch(model, loader, criterion):\n",
    "#     model.eval()\n",
    "#     running_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for i, (X_batch, y_batch) in enumerate(loader):\n",
    "\n",
    "#             X_batch = X_batch.to(device)\n",
    "#             y_batch = y_batch.to(device)\n",
    "\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "#     return running_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'entraînement étant plus long, nous choisissont d'implémenter une `patience` qui stoppera notre entraînement si en 3 itérations de suite, la val_loss n'a pas diminuée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import time\n",
    "\n",
    "# n_epochs   = 30            # máximo absoluto\n",
    "# patience   = 3             # quantas épocas ruins seguidas até parar\n",
    "# best_val   = float(\"inf\")\n",
    "# patience_counter = 0\n",
    "\n",
    "# train_losses, val_losses = [], []\n",
    "# best_model_path = \"best_mercosur.pth\"\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     start = time.time()\n",
    "\n",
    "#     train_loss = train_epoch(Multi_model, train_loader_multimodel, optimizer_Multi_model, criterion_Multi_model)\n",
    "#     val_loss   = eval_epoch(Multi_model, val_loader_multimodel, criterion_Multi_model)\n",
    "\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "\n",
    "#     epoch_time = time.time() - start\n",
    "#     print(f\"Epoch {epoch+1:03d} | {epoch_time:.1f}s | Train: {train_loss:.6f} | Val: {val_loss:.6f}\")\n",
    "\n",
    "#     # melhorou? salva modelo e zera paciência\n",
    "#     if val_loss < best_val:\n",
    "#         best_val = val_loss\n",
    "#         patience_counter = 0\n",
    "#         torch.save(capteur_model.state_dict(), best_model_path)\n",
    "#         print(f\"  Better model saved in (val_loss = {best_val:.6f})\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         print(f\" (no improvement for {patience_counter} épocas)\")\n",
    "\n",
    "#     # se não melhora há X épocas, para\n",
    "#     if patience_counter >= patience:\n",
    "#         print(f\" Early stopping in epochs {epoch+1}. Melhor val_loss = {best_val:.6f}\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Model optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant tout, on créer une fonction train qui sera utilise pour notre fontion objective à minimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def train(capteur_model, train_loader, val_loader, optimizer, criterion, epochs = 30):\n",
    "#     train_losses, val_losses = [], []\n",
    "#     for epoch in range(epochs):\n",
    "#         train_loss = train_epoch(capteur_model, train_loader, optimizer, criterion)\n",
    "#         val_loss   = eval_epoch(capteur_model, val_loader, criterion)\n",
    "#         train_losses.append(train_loss)\n",
    "#         val_losses.append(val_loss)\n",
    "#         print(f\"Epoch {epoch+1:03d} | Train: {train_loss:.6f} | Val: {val_loss:.6f}\")\n",
    "#     return capteur_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le multi modèle devrai avoir une représentation spatiale et temporelle \"juste\" du problème. par rapport au modèle précédent, la séquence d'étude du LSTM a augmentée. Nous sommes passé de 20 pas de temps à 100. Certain phénomène physique récupérés par les différents capteurs sont reliée au temps mais si l'intervalle étudiée est trop petite, il se pourrait que seulement un régime transitoire soit étudié (pas de temps de 0.032s). il reste à déterminer à quel point nos hyperparamètres sont fiable. Afin de s'en assurer, on utilise la librairie optuna, un optimiseur d'hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'optimisation se fera sur plusieurs faibles couples d'hyperparamètres afin de ne pas trop surcharger le gpu et diminuer le temps de réponse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On instancie une fonction objective1 qui donne en sortie la métrique que l'on souhaite minimiser avec l'optimiseur d'hyper paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #les initialisation de paramètres se font dans la fonction objective\n",
    "# def objective1(trial):\n",
    "#     # nb_percps_enc = trial.suggest_int(\"nb_percps_enc\",30, 60)\n",
    "#     # nb_percps_mlp = trial.suggest_int(\"nb_percps_mlp\",128, 256)\n",
    "#     # num_layersenc = trial.suggest_int(\"num_layers_enc\", 1, 5)\n",
    "#     # num_layerslstm = trial.suggest_int(\"num_layers\", 1, 5)\n",
    "#     # num_layersmlp = trial.suggest_int(\"num_layers\", 1, 5)\n",
    "#     n_features = X_train_scaled.shape[-1]\n",
    "#     n_outputs  = y_train_scaled.shape[-1]\n",
    "#     #ici on fixe des valeurs afin d'assurer un temps de réponse \"plutot\" faible. Cela serait trop couteûx de minimiser la rmse avec 8variables à déterminer\n",
    "#     nb_percps_enc = 32 \n",
    "#     nb_percps_mlp = 128\n",
    "#     num_layersenc = 3\n",
    "#     num_layerslstm = 3\n",
    "#     num_layersmlp = 3\n",
    "#     #nous porterons notre attention dans un premier temps sur z_dim et le nombre de cellules du lstm\n",
    "#     #afin de savoir quelle sont les meilleure valeurs pour caractériser spatialement et temporellement notre problème\n",
    "#     z_dim = trial.suggest_int(\"z_dim\", 10, 20)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "#     hidden_size_lstm = trial.suggest_int(\"hidden_size\", 30, 70)\n",
    "#     opti_mercosur = Mercosur(n_features, nb_percps_enc, num_layersenc, z_dim, hidden_size_lstm, num_layerslstm, nb_percps_mlp, num_layersmlp, n_outputs).to(device)\n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     opti_optimizer = torch.optim.Adam(opti_mercosur.parameters(), lr= lr)\n",
    "#     train(opti_mercosur, train_loader_multimodel_opti, val_loader_multimodel_opti, opti_optimizer, loss_fn)\n",
    "\n",
    "#     opti_mercosur.eval()\n",
    "#     y_pred_list = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for X_batch in test_loader_multimodel_opti:\n",
    "#             X_batch = X_batch.to(device)\n",
    "#             y_pred = opti_mercosur(X_batch)\n",
    "#             y_pred_list.append(y_pred.cpu().numpy())\n",
    "#     y_pred_scaled = np.concatenate(y_pred_list, axis=0)\n",
    "#     y_pred_scaled_2d = y_pred_scaled.reshape(-1, D)  # (N*T, D)\n",
    "#     y_pred_real = scaler_y.inverse_transform(y_pred_scaled_2d)\n",
    "#     rmse_global = mean_squared_error(y_test, y_pred_real, squared=False)\n",
    "#     return rmse_global #on choisira la rmse à minimiser car c'est celle demandée par le projet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on créer une étude Optuna qui récupère la fonction objective1 et qui avec n_trials essai, essaie de minimiser la rmse( et plus généralement la sortie de notre fonction prit en argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective1, n_trials = 30) #ili y aura 30 essai pour minimiser la rmse\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# print(\"Best MSE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on récupère le modèle avec la meilleure rmse. Il faut par la suite le ré-entrainé. L'étude permet seulement de récupérer les meilleurs hyperparamètre et non le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_percps_enc = 32 \n",
    "# nb_percps_mlp = 128\n",
    "# num_layersenc = 3\n",
    "# num_layerslstm = 3\n",
    "# num_layersmlp = 3\n",
    "# n_features = X_train_scaled.shape[-1]\n",
    "# n_outputs  = y_train_scaled.shape[-1]\n",
    "# best_model = study.best_trial\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# best_mercosur = Mercosur(n_features, nb_percps_enc, num_layersenc, best_model[\"z_dim\"], best_model[\"hidden_size_lstm\"], num_layerslstm, nb_percps_mlp, num_layersmlp, n_outputs).to(device)\n",
    "# loss_fn = nn.MSELoss()\n",
    "# best_optimizer = torch.optim.Adam(best_mercosur.parameters(), lr= best_model[\"lr\"])\n",
    "# train_losses_multi_opti_30epcs, test_losses_multi_opti_30epcs, final_mercosur = train(best_mercosur, train_loader_multimodel_opti, val_loader_multimodel_opti, best_optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(final_mercosur.state_dict(), \"best_mercosur.pth\")\n",
    "# print(\"model saved in 'best_mercosur.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train losses / Test losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afin de visualiser rapidement nos données, nos train et val losses des différents modèles seront récupérées dans les listes suivantes\n",
    "\n",
    "train_losses_MLP_30epc = [\n",
    "0.129939, 0.080637, 0.067185, 0.060038, 0.055594,\n",
    "0.052612, 0.050398, 0.048648, 0.047281, 0.046135,\n",
    "0.045058, 0.044317, 0.043545, 0.042896, 0.042307,\n",
    "0.041795, 0.041341, 0.040908, 0.040521, 0.040164,\n",
    "0.039814, 0.039539, 0.039237, 0.038989, 0.038781,\n",
    "0.038544, 0.038344, 0.038096, 0.037943, 0.037767\n",
    "]\n",
    "\n",
    "val_losses_MLP_30epc = [\n",
    "0.092515, 0.071673, 0.062184, 0.057984, 0.055814,\n",
    "0.052760, 0.050680, 0.048617, 0.046581, 0.045269,\n",
    "0.045962, 0.044389, 0.043543, 0.044016, 0.041793,\n",
    "0.042073, 0.042306, 0.041082, 0.041164, 0.040546,\n",
    "0.039990, 0.040046, 0.038975, 0.040417, 0.039459,\n",
    "0.038520, 0.038401, 0.039040, 0.037794, 0.040613\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "train_losses_MLP_50epc = [\n",
    "0.131692, 0.083308, 0.069435, 0.062402, 0.057915,\n",
    "0.054797, 0.052489, 0.050649, 0.049176, 0.047879,\n",
    "0.046897, 0.045973, 0.045204, 0.044569, 0.043937,\n",
    "0.043400, 0.042930, 0.042460, 0.042163, 0.041774,\n",
    "0.041434, 0.041145, 0.040847, 0.040633, 0.040362,\n",
    "0.040100, 0.039920, 0.039676, 0.039496, 0.039316,\n",
    "0.039100, 0.038929, 0.038786, 0.038627, 0.038454,\n",
    "0.038320, 0.038167, 0.038056, 0.037912, 0.037762,\n",
    "0.037704, 0.037557, 0.037451, 0.037352, 0.037290,\n",
    "0.037199, 0.037100, 0.037011, 0.036935, 0.036797\n",
    "]\n",
    "\n",
    "val_losses_MLP_50epc = [\n",
    "0.093360, 0.079029, 0.065259, 0.059254, 0.055677,\n",
    "0.052400, 0.050678, 0.050586, 0.049686, 0.048566,\n",
    "0.046253, 0.046329, 0.045473, 0.044464, 0.043709,\n",
    "0.043482, 0.042897, 0.042391, 0.042856, 0.041908,\n",
    "0.042225, 0.040851, 0.040780, 0.042736, 0.040846,\n",
    "0.040865, 0.040805, 0.042524, 0.040429, 0.040082,\n",
    "0.040065, 0.039588, 0.040238, 0.040189, 0.038365,\n",
    "0.038879, 0.039306, 0.038724, 0.038679, 0.039589,\n",
    "0.038354, 0.040551, 0.037882, 0.038679, 0.037344,\n",
    "0.037170, 0.038225, 0.038173, 0.037403, 0.037097\n",
    "]\n",
    "\n",
    "train_losses_Multimodel_24epochs = [\n",
    "    0.190224, 0.125135, 0.103884, 0.092470, 0.085805,\n",
    "    0.081463, 0.078399, 0.076031, 0.074203, 0.072751,\n",
    "    0.071560, 0.070508, 0.069678, 0.068897, 0.068235,\n",
    "    0.067663, 0.067097, 0.066567, 0.066106, 0.065716,\n",
    "    0.065320, 0.064958, 0.064680, 0.064392\n",
    "]\n",
    "\n",
    "val_losses_Multimodel_24epochs = [\n",
    "    0.140672, 0.103417, 0.086910, 0.080229, 0.074751,\n",
    "    0.065457, 0.062249, 0.060267, 0.058840, 0.057276,\n",
    "    0.057932, 0.062552, 0.057049, 0.053979, 0.051992,\n",
    "    0.052229, 0.052761, 0.051207, 0.053890, 0.049753,\n",
    "    0.048891, 0.049003, 0.051158, 0.049748\n",
    "]\n",
    "\n",
    "train_losses_Multimodel_Optm_30epochs = [\n",
    "    0.162660, 0.122851, 0.112191, 0.106104, 0.102050,\n",
    "    0.099157, 0.097028, 0.095412, 0.094011, 0.092830,\n",
    "    0.091794, 0.090849, 0.090054, 0.089326, 0.088693,\n",
    "    0.088116, 0.087583, 0.087074, 0.086616, 0.086158,\n",
    "    0.085756, 0.085369, 0.084999, 0.084672, 0.084327,\n",
    "    0.084003, 0.083735, 0.083465, 0.083245,\n",
    "]\n",
    "\n",
    "val_losses_Multimodel_Optm_30epochs = [\n",
    "    0.116050, 0.099830, 0.091637, 0.084557, 0.081131,\n",
    "    0.077814, 0.076631, 0.074812, 0.074984, 0.072687,\n",
    "    0.071368, 0.069749, 0.069939, 0.069168, 0.068984,\n",
    "    0.067767, 0.068278, 0.066779, 0.066326, 0.065510,\n",
    "    0.065594, 0.065249, 0.065012, 0.064911, 0.064201,\n",
    "    0.063369, 0.063787, 0.063319, 0.062922\n",
    "]\n",
    "\n",
    "\n",
    "train_losses_Multimodel_Optm_20epochs = [\n",
    "    0.172888, 0.129274, 0.117807, 0.111401, 0.107313,\n",
    "    0.104361, 0.101999, 0.100153, 0.098492, 0.097044,\n",
    "    0.095797, 0.094712, 0.093768, 0.092894, 0.092124,\n",
    "    0.091344, 0.090625, 0.089887, 0.089161, 0.088587,\n",
    "    0.088006\n",
    "]\n",
    "\n",
    "val_losses_Multimodel_Optm_20epochs = [\n",
    "    0.124331, 0.105276, 0.096926, 0.090205, 0.087404,\n",
    "    0.083550, 0.081730, 0.079467, 0.078049, 0.076392,\n",
    "    0.075136, 0.074486, 0.073104, 0.072052, 0.071863,\n",
    "    0.071274, 0.070129, 0.069581, 0.070083, 0.068044,\n",
    "    0.067940\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_me(train_losses, val_losses, title : str):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(epochs, val_losses,   label=\"Validation Loss\", marker='s')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig('Train_vs_Validation_Loss.png', dpi=300)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_me(train_losses_MLP_30epc, val_losses_MLP_30epc, \"MLP 30 epochs\")\n",
    "show_me(train_losses_MLP_50epc, val_losses_MLP_50epc, \"MLP 50 epochs\")\n",
    "show_me(train_losses_Multimodel_24epochs, val_losses_Multimodel_24epochs, \"Multimodel_24\")\n",
    "show_me(train_losses_Multimodel_Optm_30epochs, val_losses_Multimodel_Optm_30epochs, \"Multimodel_opti_30\")\n",
    "show_me(train_losses_Multimodel_Optm_20epochs, val_losses_Multimodel_Optm_20epochs, \"Multimodel_opti_20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_list  = [20.7183, 19.9492, 24.5641, 20.5281]\n",
    "rmse_list = [32.0403, 29.8818, 38.1646, 34.9109]\n",
    "r2_list   = [0.9592, 0.9422, 0.9084, 0.9500]\n",
    "\n",
    "metrics = [\"MAE\", \"RMSE\", \"R2\"]\n",
    "models  = [\"MLP_30epch\", \"MLP_50epch\", \"MLP_30epch_filtered\", \"Multi_model\"]\n",
    "\n",
    "df_results = pd.DataFrame(index=metrics, columns=models)\n",
    "\n",
    "df_results.loc[\"MAE\"]  = mae_list\n",
    "df_results.loc[\"RMSE\"] = rmse_list\n",
    "df_results.loc[\"R2\"]   = r2_list\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Strain_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "    y_pred_scaled = Strain_mlp(X_test_t).cpu().numpy()\n",
    "\n",
    "# Denormalizing values\n",
    "y_pred_real = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_real = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Global metrics\n",
    "mae_global  = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse_global = mean_squared_error(y_test_real, y_pred_real, squared=False)\n",
    "r2_global   = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "print(\"Overall metrics in the test set:\")\n",
    "print(f\"MAE  : {mae_global:.4f}\")\n",
    "print(f\"RMSE : {rmse_global:.4f}\")\n",
    "print(f\"R²   : {r2_global:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_flight = {}\n",
    "unique_flights = np.unique(ids_test)\n",
    "\n",
    "for flight in unique_flights:\n",
    "    mask = (ids_test == flight)\n",
    "    y_true_f = y_test_real[mask]\n",
    "    y_pred_f = y_pred_real[mask]\n",
    "\n",
    "    mae  = mean_absolute_error(y_true_f, y_pred_f)\n",
    "    rmse = mean_squared_error(y_true_f, y_pred_f, squared=False)\n",
    "    r2   = r2_score(y_true_f, y_pred_f)\n",
    "\n",
    "    metrics_per_flight[flight] = {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "for flight, m in metrics_per_flight.items():\n",
    "    print(f\"{flight}: MAE={m['MAE']:.4f}, RMSE={m['RMSE']:.4f}, R²={m['R2']:.4f}\")\n",
    "\n",
    "mae_mean  = np.mean([m[\"MAE\"]  for m in metrics_per_flight.values()])\n",
    "rmse_mean = np.mean([m[\"RMSE\"] for m in metrics_per_flight.values()])\n",
    "r2_mean   = np.mean([m[\"R2\"]   for m in metrics_per_flight.values()])\n",
    "\n",
    "print(\"\\nAverage metrics across all TOLs (test):\")\n",
    "print(f\"MAE médio  : {mae_mean:.4f}\")\n",
    "print(f\"RMSE médio : {rmse_mean:.4f}\")\n",
    "print(f\"R² médio   : {r2_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = pd.DataFrame.from_dict(metrics_per_flight, orient='index')\n",
    "\n",
    "df_global = pd.DataFrame({\n",
    "    \"MAE\":  [mae_global],\n",
    "    \"RMSE\": [rmse_global],\n",
    "    \"R2\":   [r2_global]\n",
    "}, index=[\"GLOBAL\"])\n",
    "\n",
    "df_results = pd.concat([df_flights, df_global])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('df_results_all_50epch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50  # Number of point in the x axis\n",
    "\n",
    "plt.figure(figsize=(15, 18))\n",
    "\n",
    "for i in range(15):\n",
    "    plt.subplot(5, 3, i + 1)\n",
    "    plt.plot(y_test_real[:N, i], label=\"Real\")\n",
    "    plt.plot(y_pred_real[:N, i], label=\"Predito\", alpha=0.8)\n",
    "    plt.title(f\"Strain {i+1}\")\n",
    "    plt.grid(True)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had the objective to train differents types of architectures to find which one was the best, so we used this differents architectures:\n",
    "- MLP with 30 epochs (45 min for training);\n",
    "- MLP with 30 epochs with filtering strategies (38 min for training);\n",
    "- MLP with 50 epochs (1h 30min for training);\n",
    "- Multimodel with 24 epochs (2h 10min);\n",
    "- Multimodel opmize hyperparameters (NOT DONE, maybe more than one day); \n",
    "\n",
    "The first one was the MLP, and gave us the best results, so we decided to search for others strategies to improve our score. \n",
    "\n",
    "We had the idea to optimize our hyperparameters with optuna. Nevertheless we couln't be able to let it done in time, but we had the train and the evaluation losses values of each epochs, and this gave us an idea that the model MLP could be more efficient for this case. Maybe this happened because of the size of the hidden layers that was lower than the ones used in the MLP model.\n",
    "\n",
    "Also, we used a strategie of filtering the variables and see the correlation for reduce our number of columns. We tried to get a better score, but we realized that our RMSE was greater than the model that we've trained in the beginning. \n",
    "\n",
    "To go further we could think of a PINN model, this type of architecture could include phisics representation of our data, which would have a huge impact on our results.\n",
    "\n",
    "And them the best besults we had it was:\n",
    "- MLP with 50 epochs (1h 30min for training);\n",
    "\n",
    "- RMSE = 29.8818\n",
    "- MAE = 19.9492\n",
    "- r2 = 0.9422\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Professeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "### Redefinie feature_names and target_names if your model needs it\n",
    "feature_names = ['Nz', 'Nx', 'Roll_Angle', 'Pitch_Angle', 'True_AOA',\n",
    "'True_Sideslip', 'FPA', 'True_Heading', 'CAS', 'TAS', 'Mach', 'SAT',\n",
    "'Baro_Alt', 'Roll_Rate', 'Pitch_Rate', 'Heading_Rate', 'Fuel_Qty1',\n",
    "'Fuel_Qty2', 'L_Eng_Start', 'R_Eng_Start', 'L_Throttle_Pos',\n",
    "'R_Throttle_Pos', 'L_Eng_N1', 'R_Eng_N1', 'L_Eng_N2', 'R_Eng_N2',\n",
    "'L_Gear_Down', 'R_Gear_Down', 'N_Gear_Down', 'L_Flaperon_Pos',\n",
    "'R_Flaperon_Pos', 'L_LEF_Pos', 'R_LEF_Pos', 'L_Rudder_Pos',\n",
    "'L_Stab_Pos', 'R_Stab_Pos', 'Stick_Pitch', 'Stick_Roll', 'Pedal_Pos'\n",
    "]\n",
    "target_names = ['Strain1', 'Strain2', 'Strain3', 'Strain4', 'Strain5', 'Strain6',\n",
    "'Strain7', 'Strain8', 'Strain9', 'Strain10', 'Strain11', 'Strain12',\n",
    "'Strain13', 'Strain14', 'Strain15']\n",
    "\n",
    "### Copy your model (after training)\n",
    "trained_model = deepcopy(model)\n",
    "\n",
    "### Complete with a deepcopy of your scalers if you used one\n",
    "my_scaler_X = deepcopy(scaler_X)\n",
    "my_scaler_y = deepcopy(scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "dirtest = Path(\"Data_AirLoadBench_test/\")\n",
    "test_flights = [\"TOL_16.csv\",\"TOL_45.csv\",\"TOL_32.csv\",\"TOL_47.csv\"]\n",
    "df_test = {}\n",
    "\n",
    "for flight in test_flights:\n",
    "    df_test[flight] = pd.read_csv(dirtest / flight,usecols=feature_names+target_names)\n",
    "\n",
    "### Compute performance on each flight of the test_set\n",
    "rmse = []\n",
    "r2 = []\n",
    "for flight_nb in test_flights:\n",
    "    y_test_true = df_test[flight_nb][target_names]\n",
    "    X_test = my_scaler_X.transform(df_test[flight_nb][feature_names])\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_pred_norm = trained_model(X_test)\n",
    "    y_pred = my_scaler_y.inverse_transform(y_pred_norm.detach().numpy())\n",
    "\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test_true,y_pred)))\n",
    "    r2.append(r2_score(y_test_true,y_pred))\n",
    "    print(f\"Root Mean Square Error (RMSE) on flight {flight_nb} = {rmse[-1]}\")\n",
    "    print(f\"R-square (R2) on flight {flight_nb} = {r2[-1]}\")\n",
    "\n",
    "### Print overal preformance on test set\n",
    "print('─' * 50 +\n",
    "f\"\\nOverall RMSE on test_set = {np.mean(rmse)} \\n\" +\n",
    "f\"Overall R2 on test_set = {np.mean(r2)} \\n\" +\n",
    "'─' * 50)\n",
    "\n",
    "### Plot gauge prediction\n",
    "fig, axs = plt.subplots(3,5, figsize=(25, 10))\n",
    "fig.suptitle(f\"Strain gauge prediction for flight {flight_nb}\", fontsize=14)\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "for i in range(15):\n",
    "    axs[i//5,i-(i//5)*5].plot(y_test_true[f\"Strain{i+1}\"], label=f\"True Strain{i+1}\")\n",
    "    axs[i//5,i-(i//5)*5].plot(y_pred[:,i], label=f\"True Strain{i+1}\")\n",
    "    axs[i//5,i-(i//5)*5].legend(loc = 'best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
